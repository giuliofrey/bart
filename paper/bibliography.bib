@article{chipmanBARTBayesianAdditive2010,
  title = {{{BART}}: {{Bayesian}} Additive Regression Trees},
  shorttitle = {{{BART}}},
  author = {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
  date = {2010-03-01},
  journaltitle = {The Annals of Applied Statistics},
  shortjournal = {Ann. Appl. Stat.},
  volume = {4},
  number = {1},
  eprint = {0806.3286},
  eprinttype = {arXiv},
  eprintclass = {stat},
  issn = {1932-6157},
  doi = {10.1214/09-AOAS285},
  url = {http://arxiv.org/abs/0806.3286},
  urldate = {2024-11-23},
  abstract = {We develop a Bayesian "sum-of-trees" model where each tree is constrained by a regularization prior to be a weak learner, and fitting and inference are accomplished via an iterative Bayesian backfitting MCMC algorithm that generates samples from a posterior. Effectively, BART is a nonparametric Bayesian regression approach which uses dimensionally adaptive random basis elements. Motivated by ensemble methods in general, and boosting algorithms in particular, BART is defined by a statistical model: a prior and a likelihood. This approach enables full posterior inference including point and interval estimates of the unknown regression function as well as the marginal effects of potential predictors. By keeping track of predictor inclusion frequencies, BART can also be used for model-free variable selection. BART's many features are illustrated with a bake-off against competing methods on 42 different data sets, with a simulation experiment and on a drug discovery classification problem.},
  keywords = {Bayesian Stats,Statistics - Applications,Statistics - Machine Learning,Statistics - Methodology}
}

@book{jamesIntroductionStatisticalLearning2021,
  title = {An Introduction to Statistical Learning: With Applications in {{R}}},
  shorttitle = {An Introduction to Statistical Learning},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  date = {2021},
  series = {Springer Texts in Statistics},
  edition = {Second edition},
  publisher = {Springer},
  location = {New York},
  isbn = {978-1-07-161418-1},
  langid = {english},
  pagetotal = {1}
}

@dataset{warwicknashAbalone1994a,
  title = {Abalone},
  author = {Warwick Nash, Tracy Sellers},
  date = {1994},
  publisher = {UCI Machine Learning Repository},
  doi = {10.24432/C55C7W},
  url = {https://archive.ics.uci.edu/dataset/1},
  urldate = {2024-12-07}
}


@software{mccullochBARTBayesianAdditive2024,
  title = {{{BART}}: {{Bayesian Additive Regression Trees}}},
  shorttitle = {{{BART}}},
  author = {McCulloch, Robert and Sparapani, Rodney and Gramacy, Robert and Pratola, Matthew and Spanbauer, Charles and Plummer, Martyn and Best, Nicky and Cowles, Kate and Vines, Karen},
  date = {2024-06-21},
  url = {https://cran.r-project.org/web/packages/BART/index.html},
  urldate = {2024-12-07},
  abstract = {Bayesian Additive Regression Trees (BART) provide flexible nonparametric modeling of covariates for continuous, binary, categorical and time-to-event outcomes. For more information see Sparapani, Spanbauer and McCulloch {$<$}doi:10.18637/jss.v097.i01{$>$}.},
  version = {2.9.9},
  keywords = {Bayesian,MachineLearning}
}

@software{breimanRandomForestBreimanCutlers2024,
  title = {{{randomForest}}: {{Breiman}} and {{Cutlers Random Forests}} for {{Classification}} and {{Regression}}},
  shorttitle = {{{randomForest}}},
  author = {Breiman, Leo and Cutler, Adele and Liaw, Andy and Wiener, Matthew},
  date = {2024-09-22},
  url = {https://cran.r-project.org/web/packages/randomForest/},
  urldate = {2024-12-07},
  abstract = {Classification and regression based on a forest of trees using random inputs, based on Breiman (2001) {$<$}doi:10.1023/A:1010933404324{$>$}.},
  version = {4.7-1.2},
  keywords = {Environmetrics,MachineLearning,MissingData}
}


@software{ridgewayGbmGeneralizedBoosted2024,
  title = {Gbm: {{Generalized Boosted Regression Models}}},
  shorttitle = {Gbm},
  author = {Ridgeway, Greg and Edwards, Daniel and Kriegler, Brian and Schroedl, Stefan and Southworth, Harry and Greenwell, Brandon and Boehmke, Bradley and Cunningham, Jay and Developers (https://github.com/gbm-developers), G. B. M.},
  date = {2024-06-28},
  url = {https://cran.r-project.org/web/packages/gbm/index.html},
  urldate = {2024-12-07},
  abstract = {An implementation of extensions to Freund and Schapire's AdaBoost algorithm and Friedman's gradient boosting machine. Includes regression methods for least squares, absolute loss, t-distribution loss, quantile regression, logistic, multinomial logistic, Poisson, Cox proportional hazards partial likelihood, AdaBoost exponential loss, Huberized hinge loss, and Learning to Rank measures (LambdaMart). Originally developed by Greg Ridgeway. Newer version available at github.com/gbm-developers/gbm3.},
  version = {2.2.2},
  keywords = {MachineLearning,Survival}
}

 @book{Genuer_Poggi_2020, address={Cham}, series={Use R!}, title={Random forests with R}, ISBN={9783030564858}, abstractNote={Intro -- Preface -- Acknowledgements -- Contents -- 1 Introduction to Random Forests with R -- 1.1 Preamble -- 1.2 Notation -- 1.3 Statistical Objectives -- 1.4 Packages -- 1.5 Datasets -- 1.5.1 Running Example: Spam Detection -- 1.5.2 Ozone Pollution -- 1.5.3 Genomic Data for a Vaccine Study -- 1.5.4 Dust Pollution -- 2 CART -- 2.1 The Principle -- 2.2 Maximal Tree Construction -- 2.3 Pruning -- 2.4 The rpart Package -- 2.5 Competing and Surrogate Splits -- 2.5.1 Competing Splits -- 2.5.2 Surrogate Splits -- 2.5.3 Interpretability -- 2.6 Examples -- 2.6.1 Predicting Ozone Concentration -- 2.6.2 Analyzing Genomic Data -- 3 Random Forests -- 3.1 General Principle -- 3.1.1 Instability of a Tree -- 3.1.2 From a Tree to an Ensemble: Bagging -- 3.2 Random Forest Random Inputs -- 3.3 The randomForest Package -- 3.4 Out-Of-Bag Error -- 3.5 Parameters Setting for Prediction -- 3.5.1 The number of trees ntree -- 3.5.2 The number of variables chosen at each node mtry -- 3.6 Examples -- 3.6.1 Predicting Ozone Concentration -- 3.6.2 Analyzing Genomic Data -- 3.6.3 Analyzing Dust Pollution -- 4 Variable Importance -- 4.1 Notions of Importance -- 4.2 Variable Importance Behavior -- 4.2.1 Behavior according to n and p -- 4.2.2 Behavior for Groups of Correlated Variables -- 4.3 Tree Diversity and Variables Importance -- 4.4 Influence of Parameters on Variable Importance -- 4.5 Examples -- 4.5.1 An Illustration by Simulation in Regression -- 4.5.2 Predicting Ozone Concentration -- 4.5.3 Analyzing Genomic Data -- 4.5.4 Air Pollution by Dust: What Is the Local Contribution? -- 5 Variable Selection -- 5.1 Generalities -- 5.2 Principle -- 5.3 Procedure -- 5.4 The VSURF Package -- 5.5 Setting parameters for selection -- 5.6 Examples -- 5.6.1 Predicting Ozone Concentration -- 5.6.2 Analyzing Genomic Data -- Appendix References -- -- Index}, publisher={Springer}, author={Genuer, Robin and Poggi, Jean-Michel}, year={2020}, collection={Use R!}, language={eng} }

 @article{Gordon_Breiman_Friedman_Olshen_Stone_1984, title={Classification and Regression Trees.}, volume={40}, ISSN={0006341X}, url={https://www.jstor.org/stable/2530946?origin=crossref}, DOI={10.2307/2530946}, number={3}, journal={Biometrics}, author={Gordon, A. D. and Breiman, L. and Friedman, J. H. and Olshen, R. A. and Stone, C. J.}, year={1984}, month=sep, pages={874} }

