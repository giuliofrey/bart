@article{chipmanBARTBayesianAdditive2010,
  title = {{{BART}}: {{Bayesian}} Additive Regression Trees},
  shorttitle = {{{BART}}},
  author = {Chipman, Hugh A. and George, Edward I. and McCulloch, Robert E.},
  date = {2010-03-01},
  journaltitle = {The Annals of Applied Statistics},
  shortjournal = {Ann. Appl. Stat.},
  volume = {4},
  number = {1},
  eprint = {0806.3286},
  eprinttype = {arXiv},
  eprintclass = {stat},
  issn = {1932-6157},
  doi = {10.1214/09-AOAS285},
  url = {http://arxiv.org/abs/0806.3286},
  urldate = {2024-11-23},
  abstract = {We develop a Bayesian "sum-of-trees" model where each tree is constrained by a regularization prior to be a weak learner, and fitting and inference are accomplished via an iterative Bayesian backfitting MCMC algorithm that generates samples from a posterior. Effectively, BART is a nonparametric Bayesian regression approach which uses dimensionally adaptive random basis elements. Motivated by ensemble methods in general, and boosting algorithms in particular, BART is defined by a statistical model: a prior and a likelihood. This approach enables full posterior inference including point and interval estimates of the unknown regression function as well as the marginal effects of potential predictors. By keeping track of predictor inclusion frequencies, BART can also be used for model-free variable selection. BART's many features are illustrated with a bake-off against competing methods on 42 different data sets, with a simulation experiment and on a drug discovery classification problem.},
  keywords = {Bayesian Stats,Statistics - Applications,Statistics - Machine Learning,Statistics - Methodology}
}

@book{jamesIntroductionStatisticalLearning2021,
  title = {An Introduction to Statistical Learning: With Applications in {{R}}},
  shorttitle = {An Introduction to Statistical Learning},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  date = {2021},
  series = {Springer Texts in Statistics},
  edition = {Second edition},
  publisher = {Springer},
  location = {New York},
  isbn = {978-1-07-161418-1},
  langid = {english},
  pagetotal = {1}
}

@dataset{warwicknashAbalone1994a,
  title = {Abalone},
  author = {Warwick Nash, Tracy Sellers},
  date = {1994},
  publisher = {UCI Machine Learning Repository},
  doi = {10.24432/C55C7W},
  url = {https://archive.ics.uci.edu/dataset/1},
  urldate = {2024-12-07}
}


@software{mccullochBARTBayesianAdditive2024,
  title = {{{BART}}: {{Bayesian Additive Regression Trees}}},
  shorttitle = {{{BART}}},
  author = {McCulloch, Robert and Sparapani, Rodney and Gramacy, Robert and Pratola, Matthew and Spanbauer, Charles and Plummer, Martyn and Best, Nicky and Cowles, Kate and Vines, Karen},
  date = {2024-06-21},
  url = {https://cran.r-project.org/web/packages/BART/index.html},
  urldate = {2024-12-07},
  abstract = {Bayesian Additive Regression Trees (BART) provide flexible nonparametric modeling of covariates for continuous, binary, categorical and time-to-event outcomes. For more information see Sparapani, Spanbauer and McCulloch {$<$}doi:10.18637/jss.v097.i01{$>$}.},
  version = {2.9.9},
  keywords = {Bayesian,MachineLearning}
}

@software{breimanRandomForestBreimanCutlers2024,
  title = {{{randomForest}}: {{Breiman}} and {{Cutlers Random Forests}} for {{Classification}} and {{Regression}}},
  shorttitle = {{{randomForest}}},
  author = {Breiman, Leo and Cutler, Adele and Liaw, Andy and Wiener, Matthew},
  date = {2024-09-22},
  url = {https://cran.r-project.org/web/packages/randomForest/},
  urldate = {2024-12-07},
  abstract = {Classification and regression based on a forest of trees using random inputs, based on Breiman (2001) {$<$}doi:10.1023/A:1010933404324{$>$}.},
  version = {4.7-1.2},
  keywords = {Environmetrics,MachineLearning,MissingData}
}


@software{ridgewayGbmGeneralizedBoosted2024,
  title = {Gbm: {{Generalized Boosted Regression Models}}},
  shorttitle = {Gbm},
  author = {Ridgeway, Greg and Edwards, Daniel and Kriegler, Brian and Schroedl, Stefan and Southworth, Harry and Greenwell, Brandon and Boehmke, Bradley and Cunningham, Jay and Developers (https://github.com/gbm-developers), G. B. M.},
  date = {2024-06-28},
  url = {https://cran.r-project.org/web/packages/gbm/index.html},
  urldate = {2024-12-07},
  abstract = {An implementation of extensions to Freund and Schapire's AdaBoost algorithm and Friedman's gradient boosting machine. Includes regression methods for least squares, absolute loss, t-distribution loss, quantile regression, logistic, multinomial logistic, Poisson, Cox proportional hazards partial likelihood, AdaBoost exponential loss, Huberized hinge loss, and Learning to Rank measures (LambdaMart). Originally developed by Greg Ridgeway. Newer version available at github.com/gbm-developers/gbm3.},
  version = {2.2.2},
  keywords = {MachineLearning,Survival}
}



